{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# After 500 time steps the game automaticly terminates\n",
    "\n",
    "import time\n",
    "import random\n",
    "import heapq as hp\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from collections import deque\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Dense, Dropout, Flatten, merge, Input, Lambda, merge, Activation, Embedding\n",
    "from keras.optimizers import SGD, Adam, rmsprop\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(10)\n",
    "EPISODES = 1000\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, batch_size, epsilon=1.0 ,epsilon_decay=0.98,gamma=0.99, num_atoms = 8, min_rd=-10, max_rd=500):# num_atoms = 51 since it's C51\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = {}\n",
    "        self.pqt=[]\n",
    "        self.gamma = gamma   # discount rate 0.95\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_min = 0.01   # epsilon_min = 0.01\n",
    "        self.epsilon_decay = epsilon_decay  #0.99\n",
    "        self.learning_rate = 0.0001  # the learning rate:0.001\n",
    "        self.num_atoms = num_atoms\n",
    "        self.sep=1/self.num_atoms\n",
    "        self.tau = np.array([(2*i+1)/(2*self.num_atoms) for i in range(self.num_atoms)])\n",
    "        self.model = self._build_model()\n",
    "        self.pev_model=self._build_model()\n",
    "    \n",
    "    def _EDM_loss(self, target, predicted):     \n",
    "       \n",
    "        target_tile = tf.tile(tf.reshape(target, [self.num_atoms, 1]), [1, self.num_atoms])\n",
    "        predicted_tile = tf.tile(predicted, [self.num_atoms, 1])\n",
    "        Huber_loss = tf.losses.huber_loss(target_tile, predicted_tile, reduction=tf.losses.Reduction.NONE)         \n",
    "        diff = predicted_tile - target_tile\n",
    "        tau = self.tau\n",
    "        _tau = 1.0 - tau\n",
    "        Loss = tf.where(tf.less(diff, 0.0), _tau * Huber_loss, tau * Huber_loss)\n",
    "        loss = tf.reduce_mean(tf.reduce_sum(tf.reduce_mean(Loss, axis=1), axis=0))  \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        state_input = Input(shape=(self.state_size,))\n",
    "        l1 = Dense(32, input_dim=self.state_size, activation='relu')(state_input)\n",
    "        l2 = Dense(32, activation='relu')(l1)\n",
    "        l3 = Dense(32, activation='relu')(l2)\n",
    "        \n",
    "        distribution_list = []\n",
    "        for i in range(self.action_size):\n",
    "            distribution_list.append(Dense(self.num_atoms, activation='linear')(l3))\n",
    "\n",
    "        model = Model(input=state_input, output=distribution_list)\n",
    "\n",
    "        adam = Adam(lr=self.learning_rate)\n",
    "        model.compile(loss=self._EDM_loss, optimizer=adam)\n",
    "       \n",
    "        return model\n",
    "    \n",
    "    def remember(self, eps, state, action, reward, next_state, done):\n",
    "        \n",
    "        \n",
    "        Quantiles_medians = self.model.predict(state)        \n",
    "        Quantiles_act_median = Quantiles_medians[action][0]        \n",
    "        prd_rwd = np.mean(Quantiles_act_median)    \n",
    "        Quantiles_medians_next = self.pev_model.predict(next_state)\n",
    "        Qsa = [sum(Quantile[0]) for Quantile in Quantiles_medians_next]\n",
    "        corr_rwd = reward + self.gamma*Qsa[np.argmax(Qsa)]\n",
    "        \n",
    "        hp.heappush(self.pqt,-abs(float(prd_rwd - corr_rwd)))\n",
    "        self.memory[-abs(float(prd_rwd - corr_rwd))]=(state, action, reward, next_state, done)\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        pQs = self.model.predict(state)\n",
    "        Qsa = [sum(pQsa[0]) for pQsa in pQs]\n",
    "        return np.argmax(Qsa) # returns action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "    \n",
    "        for _ in range(batch_size):\n",
    "            \n",
    "            state, action, reward, next_state, done = self.memory[hp.heappop(self.pqt)]  \n",
    "            if done:    \n",
    "                target = np.ones(self.num_atoms)*reward\n",
    "        \n",
    "            if not done:\n",
    "        \n",
    "                Quantiles_medians_next = self.pev_model.predict(next_state)\n",
    "                Qsa = [sum(pQsa[0]) for pQsa in Quantiles_medians_next]\n",
    "                corr_rwd = reward + self.gamma*Quantiles_medians_next[np.argmax(Qsa)][0]\n",
    "                target = corr_rwd\n",
    "    \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[action][0] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    time_start=time.time()\n",
    "    batch_size = 32\n",
    "    agent = DQNAgent(state_size, action_size, batch_size)\n",
    "    done = False\n",
    "    \n",
    "    vally=0\n",
    "    rslt=[]\n",
    "    ep=[]\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        \n",
    "        done=False\n",
    "        state = env.reset()  # It's obdervation 'o'\n",
    "        state = np.reshape(state, [1, state_size]) # Encapsulating whole thing into and array i.e [[1,12,2,3]]\n",
    "        for t in range(500):   \n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = 0 if t==499 or not done else -1\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(e, state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "              \n",
    "            if done and not t==499:\n",
    "                agent.pev_model.set_weights(agent.model.get_weights())\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, EPISODES, t, agent.epsilon))\n",
    "                break\n",
    "        rslt.append(t)\n",
    "        if t>=499 : \n",
    "            vally+=1\n",
    "            ep.append(e)\n",
    "            print(\"Time for which pole stand:\",t,vally)\n",
    "            if len(ep)>2:\n",
    "                \n",
    "                if list(range(3))==[smt-ep[-3] for smt in ep[-3:]]:\n",
    "                    print(\"Done after episode:\",e-1)\n",
    "                    break\n",
    "                 \n",
    "        if len(agent.pqt) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "    end_time=time.time()\n",
    "    print(end_time-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
