{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is for the cart-pole game, it's all about my experiments with the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0/1000, score: 18, e: 1.0\n",
      "episode: 1/1000, score: 27, e: 1.0\n",
      "episode: 2/1000, score: 24, e: 1.0\n",
      "episode: 3/1000, score: 17, e: 0.99\n",
      "episode: 4/1000, score: 12, e: 0.99\n",
      "episode: 5/1000, score: 23, e: 0.99\n",
      "episode: 6/1000, score: 14, e: 0.99\n",
      "episode: 7/1000, score: 18, e: 0.98\n",
      "episode: 8/1000, score: 11, e: 0.98\n",
      "episode: 9/1000, score: 26, e: 0.98\n",
      "episode: 10/1000, score: 17, e: 0.97\n",
      "episode: 11/1000, score: 27, e: 0.97\n",
      "episode: 12/1000, score: 11, e: 0.97\n",
      "episode: 13/1000, score: 50, e: 0.96\n",
      "episode: 14/1000, score: 12, e: 0.96\n",
      "episode: 15/1000, score: 24, e: 0.95\n",
      "episode: 16/1000, score: 15, e: 0.95\n",
      "episode: 17/1000, score: 29, e: 0.95\n",
      "episode: 18/1000, score: 34, e: 0.94\n",
      "episode: 19/1000, score: 16, e: 0.94\n",
      "episode: 20/1000, score: 10, e: 0.94\n",
      "episode: 21/1000, score: 17, e: 0.93\n",
      "episode: 22/1000, score: 9, e: 0.93\n",
      "episode: 23/1000, score: 19, e: 0.93\n",
      "episode: 24/1000, score: 9, e: 0.93\n",
      "episode: 25/1000, score: 20, e: 0.92\n",
      "episode: 26/1000, score: 12, e: 0.92\n",
      "episode: 27/1000, score: 14, e: 0.92\n",
      "episode: 28/1000, score: 11, e: 0.92\n",
      "episode: 29/1000, score: 16, e: 0.92\n",
      "episode: 30/1000, score: 30, e: 0.91\n",
      "episode: 31/1000, score: 19, e: 0.91\n",
      "episode: 32/1000, score: 12, e: 0.9\n",
      "episode: 33/1000, score: 28, e: 0.9\n",
      "episode: 34/1000, score: 31, e: 0.9\n",
      "episode: 35/1000, score: 12, e: 0.9\n",
      "episode: 36/1000, score: 9, e: 0.9\n",
      "episode: 37/1000, score: 24, e: 0.9\n",
      "episode: 38/1000, score: 25, e: 0.9\n",
      "episode: 39/1000, score: 30, e: 0.89\n",
      "episode: 40/1000, score: 10, e: 0.89\n",
      "episode: 41/1000, score: 12, e: 0.88\n",
      "episode: 42/1000, score: 29, e: 0.88\n",
      "episode: 43/1000, score: 11, e: 0.88\n",
      "episode: 44/1000, score: 11, e: 0.88\n",
      "episode: 45/1000, score: 16, e: 0.87\n",
      "episode: 46/1000, score: 16, e: 0.87\n",
      "episode: 47/1000, score: 15, e: 0.87\n",
      "episode: 48/1000, score: 23, e: 0.87\n",
      "episode: 49/1000, score: 13, e: 0.86\n",
      "episode: 50/1000, score: 11, e: 0.86\n",
      "episode: 51/1000, score: 12, e: 0.86\n",
      "episode: 52/1000, score: 15, e: 0.86\n",
      "episode: 53/1000, score: 12, e: 0.85\n",
      "episode: 54/1000, score: 14, e: 0.85\n",
      "episode: 55/1000, score: 37, e: 0.85\n",
      "episode: 56/1000, score: 11, e: 0.84\n",
      "episode: 57/1000, score: 58, e: 0.84\n",
      "episode: 58/1000, score: 9, e: 0.83\n",
      "episode: 59/1000, score: 10, e: 0.83\n",
      "episode: 60/1000, score: 12, e: 0.83\n",
      "episode: 61/1000, score: 11, e: 0.83\n",
      "episode: 62/1000, score: 16, e: 0.83\n",
      "episode: 63/1000, score: 25, e: 0.83\n",
      "episode: 64/1000, score: 14, e: 0.83\n",
      "episode: 65/1000, score: 25, e: 0.83\n",
      "episode: 66/1000, score: 11, e: 0.82\n",
      "episode: 67/1000, score: 14, e: 0.82\n",
      "episode: 68/1000, score: 24, e: 0.82\n",
      "episode: 69/1000, score: 27, e: 0.81\n",
      "episode: 70/1000, score: 19, e: 0.81\n",
      "episode: 71/1000, score: 7, e: 0.81\n",
      "episode: 72/1000, score: 11, e: 0.81\n",
      "episode: 73/1000, score: 26, e: 0.8\n",
      "episode: 74/1000, score: 15, e: 0.8\n",
      "episode: 75/1000, score: 18, e: 0.8\n",
      "episode: 76/1000, score: 10, e: 0.79\n",
      "episode: 77/1000, score: 15, e: 0.79\n",
      "episode: 78/1000, score: 11, e: 0.79\n",
      "episode: 79/1000, score: 10, e: 0.79\n",
      "episode: 80/1000, score: 10, e: 0.79\n",
      "episode: 81/1000, score: 9, e: 0.79\n",
      "episode: 82/1000, score: 10, e: 0.79\n",
      "episode: 83/1000, score: 25, e: 0.79\n",
      "episode: 84/1000, score: 20, e: 0.79\n",
      "episode: 85/1000, score: 36, e: 0.78\n",
      "episode: 86/1000, score: 14, e: 0.78\n",
      "episode: 87/1000, score: 14, e: 0.78\n",
      "episode: 88/1000, score: 35, e: 0.77\n",
      "episode: 89/1000, score: 12, e: 0.77\n",
      "episode: 90/1000, score: 12, e: 0.77\n",
      "episode: 91/1000, score: 32, e: 0.76\n",
      "episode: 92/1000, score: 53, e: 0.76\n",
      "episode: 93/1000, score: 24, e: 0.75\n",
      "episode: 94/1000, score: 13, e: 0.75\n",
      "episode: 95/1000, score: 11, e: 0.75\n",
      "episode: 96/1000, score: 13, e: 0.75\n",
      "episode: 97/1000, score: 9, e: 0.75\n",
      "episode: 98/1000, score: 22, e: 0.75\n",
      "episode: 99/1000, score: 55, e: 0.74\n",
      "episode: 100/1000, score: 15, e: 0.74\n",
      "episode: 101/1000, score: 34, e: 0.73\n",
      "episode: 102/1000, score: 13, e: 0.73\n",
      "episode: 103/1000, score: 12, e: 0.73\n",
      "episode: 104/1000, score: 24, e: 0.72\n",
      "episode: 105/1000, score: 9, e: 0.72\n",
      "episode: 106/1000, score: 11, e: 0.72\n",
      "episode: 107/1000, score: 15, e: 0.72\n",
      "episode: 108/1000, score: 12, e: 0.72\n",
      "episode: 109/1000, score: 13, e: 0.72\n",
      "episode: 110/1000, score: 12, e: 0.72\n",
      "episode: 111/1000, score: 8, e: 0.72\n",
      "episode: 112/1000, score: 11, e: 0.72\n",
      "episode: 113/1000, score: 32, e: 0.71\n",
      "episode: 114/1000, score: 37, e: 0.71\n",
      "episode: 115/1000, score: 47, e: 0.7\n",
      "episode: 116/1000, score: 64, e: 0.7\n",
      "episode: 117/1000, score: 48, e: 0.7\n",
      "episode: 118/1000, score: 48, e: 0.69\n",
      "episode: 119/1000, score: 27, e: 0.68\n",
      "episode: 120/1000, score: 15, e: 0.68\n",
      "episode: 121/1000, score: 16, e: 0.68\n",
      "episode: 122/1000, score: 29, e: 0.68\n",
      "episode: 123/1000, score: 16, e: 0.68\n",
      "episode: 124/1000, score: 15, e: 0.67\n",
      "episode: 125/1000, score: 12, e: 0.67\n",
      "episode: 126/1000, score: 39, e: 0.67\n",
      "episode: 127/1000, score: 33, e: 0.66\n",
      "episode: 128/1000, score: 18, e: 0.66\n",
      "episode: 129/1000, score: 32, e: 0.66\n",
      "episode: 130/1000, score: 28, e: 0.66\n",
      "episode: 131/1000, score: 22, e: 0.65\n",
      "episode: 132/1000, score: 15, e: 0.65\n",
      "episode: 133/1000, score: 17, e: 0.65\n",
      "episode: 134/1000, score: 55, e: 0.65\n",
      "episode: 135/1000, score: 19, e: 0.64\n",
      "episode: 136/1000, score: 20, e: 0.64\n",
      "episode: 137/1000, score: 58, e: 0.64\n",
      "episode: 138/1000, score: 87, e: 0.63\n",
      "episode: 139/1000, score: 115, e: 0.62\n",
      "episode: 140/1000, score: 39, e: 0.62\n",
      "episode: 141/1000, score: 11, e: 0.61\n",
      "episode: 142/1000, score: 61, e: 0.61\n",
      "episode: 143/1000, score: 20, e: 0.6\n",
      "episode: 144/1000, score: 22, e: 0.6\n",
      "episode: 145/1000, score: 15, e: 0.6\n",
      "episode: 146/1000, score: 42, e: 0.59\n",
      "episode: 147/1000, score: 19, e: 0.59\n",
      "episode: 148/1000, score: 19, e: 0.59\n",
      "episode: 149/1000, score: 24, e: 0.59\n",
      "episode: 150/1000, score: 34, e: 0.59\n",
      "episode: 151/1000, score: 12, e: 0.58\n",
      "episode: 152/1000, score: 24, e: 0.58\n",
      "episode: 153/1000, score: 14, e: 0.58\n",
      "episode: 154/1000, score: 18, e: 0.58\n",
      "episode: 155/1000, score: 16, e: 0.58\n",
      "episode: 156/1000, score: 11, e: 0.58\n",
      "episode: 157/1000, score: 22, e: 0.57\n",
      "episode: 158/1000, score: 38, e: 0.57\n",
      "episode: 159/1000, score: 46, e: 0.56\n",
      "episode: 160/1000, score: 15, e: 0.56\n",
      "episode: 161/1000, score: 17, e: 0.56\n",
      "episode: 162/1000, score: 10, e: 0.56\n",
      "episode: 163/1000, score: 16, e: 0.56\n",
      "episode: 164/1000, score: 204, e: 0.56\n",
      "episode: 165/1000, score: 20, e: 0.55\n",
      "episode: 166/1000, score: 40, e: 0.55\n",
      "episode: 167/1000, score: 57, e: 0.54\n",
      "episode: 168/1000, score: 32, e: 0.54\n",
      "episode: 169/1000, score: 28, e: 0.53\n",
      "episode: 170/1000, score: 94, e: 0.53\n",
      "episode: 171/1000, score: 22, e: 0.52\n",
      "episode: 172/1000, score: 28, e: 0.52\n",
      "episode: 173/1000, score: 21, e: 0.51\n",
      "episode: 174/1000, score: 22, e: 0.51\n",
      "episode: 175/1000, score: 17, e: 0.51\n",
      "episode: 176/1000, score: 32, e: 0.5\n",
      "episode: 177/1000, score: 58, e: 0.5\n",
      "episode: 178/1000, score: 40, e: 0.5\n",
      "episode: 179/1000, score: 27, e: 0.49\n",
      "episode: 180/1000, score: 15, e: 0.49\n",
      "episode: 181/1000, score: 29, e: 0.49\n",
      "episode: 182/1000, score: 116, e: 0.49\n",
      "episode: 183/1000, score: 28, e: 0.48\n",
      "episode: 184/1000, score: 116, e: 0.48\n",
      "episode: 185/1000, score: 24, e: 0.48\n",
      "episode: 186/1000, score: 19, e: 0.47\n",
      "episode: 187/1000, score: 51, e: 0.47\n",
      "episode: 188/1000, score: 28, e: 0.47\n",
      "episode: 189/1000, score: 27, e: 0.46\n",
      "episode: 190/1000, score: 144, e: 0.46\n",
      "episode: 191/1000, score: 28, e: 0.46\n",
      "episode: 192/1000, score: 136, e: 0.45\n",
      "episode: 193/1000, score: 132, e: 0.45\n",
      "episode: 194/1000, score: 143, e: 0.44\n",
      "episode: 195/1000, score: 129, e: 0.44\n",
      "episode: 196/1000, score: 416, e: 0.43\n",
      "episode: 197/1000, score: 203, e: 0.43\n",
      "episode: 198/1000, score: 171, e: 0.43\n",
      "episode: 199/1000, score: 265, e: 0.42\n",
      "episode: 200/1000, score: 79, e: 0.42\n",
      "episode: 201/1000, score: 60, e: 0.41\n",
      "episode: 202/1000, score: 46, e: 0.41\n",
      "episode: 203/1000, score: 158, e: 0.4\n",
      "episode: 204/1000, score: 412, e: 0.4\n",
      "episode: 205/1000, score: 48, e: 0.4\n",
      "episode: 206/1000, score: 199, e: 0.39\n",
      "episode: 207/1000, score: 123, e: 0.39\n",
      "episode: 208/1000, score: 161, e: 0.38\n",
      "episode: 209/1000, score: 200, e: 0.38\n",
      "episode: 210/1000, score: 398, e: 0.38\n",
      "episode: 211/1000, score: 320, e: 0.37\n",
      "episode: 212/1000, score: 108, e: 0.37\n",
      "episode: 213/1000, score: 140, e: 0.37\n",
      "episode: 214/1000, score: 35, e: 0.36\n",
      "episode: 215/1000, score: 124, e: 0.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 216/1000, score: 204, e: 0.36\n",
      "episode: 217/1000, score: 205, e: 0.35\n",
      "episode: 218/1000, score: 143, e: 0.35\n",
      "episode: 219/1000, score: 142, e: 0.34\n",
      "episode: 220/1000, score: 385, e: 0.34\n",
      "episode: 221/1000, score: 499, e: 0.34\n",
      "Time for which pole stand: 499 1\n",
      "episode: 222/1000, score: 165, e: 0.33\n",
      "episode: 223/1000, score: 190, e: 0.33\n",
      "episode: 224/1000, score: 246, e: 0.33\n",
      "episode: 225/1000, score: 216, e: 0.32\n",
      "episode: 226/1000, score: 146, e: 0.32\n",
      "episode: 227/1000, score: 186, e: 0.32\n",
      "episode: 228/1000, score: 169, e: 0.31\n",
      "episode: 229/1000, score: 276, e: 0.31\n",
      "episode: 230/1000, score: 76, e: 0.31\n",
      "episode: 231/1000, score: 306, e: 0.31\n",
      "episode: 232/1000, score: 318, e: 0.3\n",
      "episode: 233/1000, score: 376, e: 0.3\n",
      "episode: 234/1000, score: 383, e: 0.3\n",
      "episode: 235/1000, score: 208, e: 0.29\n",
      "episode: 236/1000, score: 299, e: 0.29\n",
      "episode: 237/1000, score: 221, e: 0.29\n",
      "episode: 238/1000, score: 499, e: 0.28\n",
      "Time for which pole stand: 499 2\n",
      "episode: 239/1000, score: 414, e: 0.28\n",
      "episode: 240/1000, score: 328, e: 0.28\n",
      "episode: 241/1000, score: 106, e: 0.28\n",
      "episode: 242/1000, score: 285, e: 0.27\n",
      "episode: 243/1000, score: 499, e: 0.27\n",
      "Time for which pole stand: 499 3\n",
      "episode: 244/1000, score: 499, e: 0.27\n",
      "Time for which pole stand: 499 4\n",
      "episode: 245/1000, score: 499, e: 0.27\n",
      "Time for which pole stand: 499 5\n",
      "episode: 246/1000, score: 349, e: 0.26\n",
      "episode: 247/1000, score: 499, e: 0.26\n",
      "Time for which pole stand: 499 6\n",
      "episode: 248/1000, score: 78, e: 0.26\n",
      "episode: 249/1000, score: 452, e: 0.25\n",
      "episode: 250/1000, score: 499, e: 0.25\n",
      "Time for which pole stand: 499 7\n",
      "episode: 251/1000, score: 379, e: 0.25\n",
      "episode: 252/1000, score: 238, e: 0.25\n",
      "episode: 253/1000, score: 212, e: 0.24\n",
      "episode: 254/1000, score: 308, e: 0.24\n",
      "episode: 255/1000, score: 324, e: 0.24\n",
      "episode: 256/1000, score: 235, e: 0.24\n",
      "episode: 257/1000, score: 499, e: 0.24\n",
      "Time for which pole stand: 499 8\n",
      "episode: 258/1000, score: 499, e: 0.23\n",
      "Time for which pole stand: 499 9\n",
      "episode: 259/1000, score: 98, e: 0.23\n",
      "episode: 260/1000, score: 205, e: 0.23\n",
      "episode: 261/1000, score: 275, e: 0.23\n",
      "episode: 262/1000, score: 256, e: 0.22\n",
      "episode: 263/1000, score: 499, e: 0.22\n",
      "Time for which pole stand: 499 10\n",
      "73.9700882434845\n"
     ]
    }
   ],
   "source": [
    "# After 500 time steps the game automaticly terminates\n",
    "\n",
    "import time\n",
    "import random\n",
    "import heapq as hp\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size,epsilon_decay=.99,gamma=0.95):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = {}\n",
    "        self.pqt=[]\n",
    "        self.gamma = gamma   # discount rate 0.95\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = epsilon_decay  #0.99\n",
    "        self.learning_rate = 0.001  # the learning rate\n",
    "        self.model = self._build_model()\n",
    "        self.pev_model=self.model        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        cort_rwd=(reward + self.gamma *\n",
    "                          (self.pev_model.predict(next_state)[0])[self.act_unbaised(next_state)])\n",
    "        prd_rwd=self.model.predict(next_state)[0][action]\n",
    "        hp.heappush(self.pqt,-abs(float(cort_rwd - prd_rwd)))\n",
    "        self.memory[-abs(float(cort_rwd - prd_rwd))]=(state, action, reward, next_state, done)\n",
    "        \n",
    "    \n",
    "    def act_unbaised(self,state):\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            \n",
    "            state, action, reward, next_state, done = self.memory[hp.heappop(self.pqt)]\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.pev_model.predict(next_state)[0]))  # q(s,a) = r + max(q(_s,_a)) over _a\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    time_start=time.time()\n",
    "    \n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    # agent.load(\"./save/cartpole-dqn.h5\")\n",
    "    done = False\n",
    "    batch_size = 64\n",
    "    vally=0\n",
    "    for e in range(EPISODES):\n",
    "        \n",
    "        done=False\n",
    "        state = env.reset()  # It's obdervation 'o'\n",
    "        state = np.reshape(state, [1, state_size]) # Encapsulating whole thing into and array i.e [[1,12,2,3]]\n",
    "        for t in range(500):      # The 1st comment, actually 'for' loop will work too\n",
    "            \n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, EPISODES, t, agent.epsilon))\n",
    "                break\n",
    "        if t>=499 : \n",
    "            vally+=1\n",
    "            print(\"Time for which pole stand:\",t,vally)\n",
    "            if vally==10:\n",
    "                break\n",
    "                print(\"Done after episode:\",e-1)\n",
    "        if len(agent.pqt) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        agent.pev_model=agent.model\n",
    "    end_time=time.time()\n",
    "    print(end_time-time_start)\n",
    "        # if e % 10 == 0:\n",
    "#     agent.save(\"./save/cartpole-dqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.memory.clear()\n",
    "agent.pqt.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' for the normal one: episode 541 Avg : 374.64 nxt time(same kernal) episode 394 avg 499\\n    nxt time(diff_kernal) episode : deoest converge avg:146.97\\n    nxt time(diff_kernal) episode : 720 avg:389.93\\n# epsilon_decay=0.99\\n    nxt time(diff_kernal) episode : 399 avg:290.88'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Time took = 187.36419129371643 , 231.45736050605774\n",
    "# After episode 367 time : 55.953306913375854 Avg evaluation : 479.72 for vally=5\n",
    "\"\"\"For epsilon_Decay=0.98\n",
    "    Episode 413 time 93.76821875572205\n",
    "    Avg evavluation : 499\n",
    "    for vally=10 time: 104.12895131111145\"\"\"\n",
    "# now vally=10 \n",
    "''' After episode 396 time : 101.89768052101135 Avg : 422.85,\n",
    "    nxt time episode 386 time : 117.12235856056213 avg 485.68\n",
    "    nxt time episode 503 time : 211.13185095787048 avg 438.84\n",
    "# epsilon_decay=0.99\n",
    "    nxt time episode 283 time : 58.04395508766174 avg  498.18\n",
    "    nxt time episode 234 time : 46.76658082008362 avg  492.76\n",
    "    nxt time episode 359 time : 96.6677758693695 avg  499.00\n",
    "# epsilon_decay=0.995\n",
    "    nxt time episode 395 time : 87.64711785316467 avg  491.93\n",
    "    nxt time episode 479 time : 152.34307408332825 avg  490.36\n",
    "    nxt time episode 397 time : 84.11703038215637 avg  499.00'''\n",
    "# epsilon_decay=0.995\n",
    "''' for the normal one: episode 541 Avg : 374.64 nxt time(same kernal) episode 394 avg 499\n",
    "    nxt time(diff_kernal) episode : deoest converge avg:146.97\n",
    "    nxt time(diff_kernal) episode : 720 avg:389.93\n",
    "# epsilon_decay=0.99\n",
    "    nxt time(diff_kernal) episode : 399 avg:290.88'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env,agent):\n",
    "    avg=0\n",
    "    for i in range(100):\n",
    "        #env.seed(i)\n",
    "        state=env.reset()\n",
    "        state=np.reshape(state,[1,agent.state_size])\n",
    "        action=np.argmax(agent.model.predict(state)[0])\n",
    "        nxt_state, reward, done, info = env.step(action)\n",
    "        net_r=0\n",
    "        while not done:\n",
    "            net_r+=reward\n",
    "            state=nxt_state\n",
    "            state=np.reshape(state, [1,agent.state_size])\n",
    "            action=np.argmax(agent.model.predict(state)[0])\n",
    "            nxt_state, reward, done, info = env.step(action)\n",
    "            #print(nxt_state, reward, done)\n",
    "        avg+=net_r\n",
    "        print(\"Reward for {} episode: {}\".format(i,net_r))\n",
    "    print(\"Avg reward: \",avg/(i+1) )\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for 0 episode: 165.0\n",
      "Reward for 1 episode: 499.0\n",
      "Reward for 2 episode: 458.0\n",
      "Reward for 3 episode: 499.0\n",
      "Reward for 4 episode: 179.0\n",
      "Reward for 5 episode: 499.0\n",
      "Reward for 6 episode: 499.0\n",
      "Reward for 7 episode: 499.0\n",
      "Reward for 8 episode: 297.0\n",
      "Reward for 9 episode: 293.0\n",
      "Reward for 10 episode: 499.0\n",
      "Reward for 11 episode: 313.0\n",
      "Reward for 12 episode: 499.0\n",
      "Reward for 13 episode: 499.0\n",
      "Reward for 14 episode: 357.0\n",
      "Reward for 15 episode: 499.0\n",
      "Reward for 16 episode: 467.0\n",
      "Reward for 17 episode: 492.0\n",
      "Reward for 18 episode: 335.0\n",
      "Reward for 19 episode: 461.0\n",
      "Reward for 20 episode: 304.0\n",
      "Reward for 21 episode: 499.0\n",
      "Reward for 22 episode: 499.0\n",
      "Reward for 23 episode: 499.0\n",
      "Reward for 24 episode: 499.0\n",
      "Reward for 25 episode: 373.0\n",
      "Reward for 26 episode: 499.0\n",
      "Reward for 27 episode: 499.0\n",
      "Reward for 28 episode: 481.0\n",
      "Reward for 29 episode: 195.0\n",
      "Reward for 30 episode: 499.0\n",
      "Reward for 31 episode: 499.0\n",
      "Reward for 32 episode: 304.0\n",
      "Reward for 33 episode: 180.0\n",
      "Reward for 34 episode: 475.0\n",
      "Reward for 35 episode: 499.0\n",
      "Reward for 36 episode: 499.0\n",
      "Reward for 37 episode: 499.0\n",
      "Reward for 38 episode: 499.0\n",
      "Reward for 39 episode: 499.0\n",
      "Reward for 40 episode: 454.0\n",
      "Reward for 41 episode: 153.0\n",
      "Reward for 42 episode: 499.0\n",
      "Reward for 43 episode: 266.0\n",
      "Reward for 44 episode: 337.0\n",
      "Reward for 45 episode: 499.0\n",
      "Reward for 46 episode: 365.0\n",
      "Reward for 47 episode: 377.0\n",
      "Reward for 48 episode: 499.0\n",
      "Reward for 49 episode: 335.0\n",
      "Reward for 50 episode: 499.0\n",
      "Reward for 51 episode: 499.0\n",
      "Reward for 52 episode: 499.0\n",
      "Reward for 53 episode: 470.0\n",
      "Reward for 54 episode: 159.0\n",
      "Reward for 55 episode: 380.0\n",
      "Reward for 56 episode: 333.0\n",
      "Reward for 57 episode: 116.0\n",
      "Reward for 58 episode: 499.0\n",
      "Reward for 59 episode: 499.0\n",
      "Reward for 60 episode: 298.0\n",
      "Reward for 61 episode: 499.0\n",
      "Reward for 62 episode: 499.0\n",
      "Reward for 63 episode: 499.0\n",
      "Reward for 64 episode: 499.0\n",
      "Reward for 65 episode: 311.0\n",
      "Reward for 66 episode: 499.0\n",
      "Reward for 67 episode: 331.0\n",
      "Reward for 68 episode: 143.0\n",
      "Reward for 69 episode: 499.0\n",
      "Reward for 70 episode: 499.0\n",
      "Reward for 71 episode: 499.0\n",
      "Reward for 72 episode: 499.0\n",
      "Reward for 73 episode: 319.0\n",
      "Reward for 74 episode: 301.0\n",
      "Reward for 75 episode: 467.0\n",
      "Reward for 76 episode: 499.0\n",
      "Reward for 77 episode: 328.0\n",
      "Reward for 78 episode: 499.0\n",
      "Reward for 79 episode: 413.0\n",
      "Reward for 80 episode: 499.0\n",
      "Reward for 81 episode: 319.0\n",
      "Reward for 82 episode: 468.0\n",
      "Reward for 83 episode: 383.0\n",
      "Reward for 84 episode: 499.0\n",
      "Reward for 85 episode: 466.0\n",
      "Reward for 86 episode: 371.0\n",
      "Reward for 87 episode: 398.0\n",
      "Reward for 88 episode: 499.0\n",
      "Reward for 89 episode: 462.0\n",
      "Reward for 90 episode: 499.0\n",
      "Reward for 91 episode: 309.0\n",
      "Reward for 92 episode: 499.0\n",
      "Reward for 93 episode: 333.0\n",
      "Reward for 94 episode: 353.0\n",
      "Reward for 95 episode: 499.0\n",
      "Reward for 96 episode: 489.0\n",
      "Reward for 97 episode: 372.0\n",
      "Reward for 98 episode: 499.0\n",
      "Reward for 99 episode: 341.0\n",
      "Avg reward:  418.01\n"
     ]
    }
   ],
   "source": [
    "policy_evaluation(env,agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import heapq as hp\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size,epsilon_decay,gamma):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = {}\n",
    "        self.pqt=[]\n",
    "        self.gamma = gamma   # discount rate 0.95\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = epsilon_decay  #0.99\n",
    "        self.learning_rate = 0.001  # the learning rate\n",
    "        self.model = self._build_model()\n",
    "        self.pev_model=self.model        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        cort_rwd=(reward + self.gamma *\n",
    "                          (self.pev_model.predict(next_state)[0])[self.act_unbaised(next_state)])\n",
    "        prd_rwd=self.model.predict(next_state)[0][action]\n",
    "        hp.heappush(self.pqt,-abs(float(cort_rwd - prd_rwd)))\n",
    "        self.memory[-abs(float(cort_rwd - prd_rwd))]=(state, action, reward, next_state, done)\n",
    "        \n",
    "    \n",
    "    def act_unbaised(self,state):\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            \n",
    "            state, action, reward, next_state, done = self.memory[hp.heappop(self.pqt)]\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.pev_model.predict(next_state)[0]))  # q(s,a) = r + max(q(_s,_a)) over _a\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After 500 time steps the game automaticly terminates\n",
    "\n",
    "\n",
    "\n",
    "def do_it(agent):\n",
    "    env.reset()\n",
    "    time_start=time.time()\n",
    "    \n",
    "    \n",
    "    # agent.load(\"./save/cartpole-dqn.h5\")\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    vally=[]\n",
    "    for e in range(EPISODES):\n",
    "\n",
    "        done=False\n",
    "        state = env.reset()  # It's obdervation 'o'\n",
    "        state = np.reshape(state, [1, state_size]) # Encapsulating whole thing into and array i.e [[1,12,2,3]]\n",
    "        for t in range(500):      # The 1st comment, actually 'for' loop will work too\n",
    "            \n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "           # agent.pev_model=agent.model  # I think this should be episidic not for every t\n",
    "            if done:\n",
    "               # print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                #      .format(e, EPISODES, t, agent.epsilon))\n",
    "                break\n",
    "        if t>=499 : \n",
    "            vally.append(e-1)\n",
    "            #print(\"Time for which pole stand:\",t,vally)\n",
    "            if len(vally)==10:\n",
    "                break\n",
    "                \n",
    "        if len(agent.pqt) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        agent.pev_model=agent.model\n",
    "    end_time=time.time()\n",
    "    \n",
    "    \n",
    "    avg=0\n",
    "    for i in range(100):\n",
    "        #env.seed(i)\n",
    "        state=env.reset()\n",
    "        state=np.reshape(state,[1,agent.state_size])\n",
    "        action=np.argmax(agent.model.predict(state)[0])\n",
    "        nxt_state, reward, done, info = env.step(action)\n",
    "        net_r=0\n",
    "        while not done:\n",
    "            net_r+=reward\n",
    "            state=nxt_state\n",
    "            state=np.reshape(state, [1,agent.state_size])\n",
    "            action=np.argmax(agent.model.predict(state)[0])\n",
    "            nxt_state, reward, done, info = env.step(action)\n",
    "            #print(nxt_state, reward, done)\n",
    "        avg+=net_r\n",
    "        #print(\"Reward for {} episode: {}\".format(i,net_r))\n",
    "    \n",
    "    agent.memory.clear()\n",
    "    agent.pqt.clear()\n",
    "    \n",
    "    return([end_time-time_start, vally, avg/(i+1), agent])\n",
    "        # if e % 10 == 0:\n",
    "#     agent.save(\"./save/cartpole-dqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set some hyperparameters gamma, epsilon_decay,min_decay\n",
    "gammaLst=[0.8,0.85,0.9,0.95,1]\n",
    "epsilon_decayLst=[0.5,0.7,0.8,0.9,0.95,0.98,0.985,0.99,0.995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent_hyp=[]\n",
    "for i in range(len(gammaLst)):\n",
    "    agent_gamma=[]\n",
    "    for j in range(len(epsilon_decayLst)):\n",
    "        finl=[]\n",
    "        for po in range(3):\n",
    "            finl.append(DQNAgent(state_size, action_size,epsilon_decayLst[j],gammaLst[i]))\n",
    "        agent_gamma.append(finl)\n",
    "    agent_hyp.append(agent_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma :  0.8\n",
      "     Epsilon_Decay :  0.5\n",
      "            Trial :  1\n",
      "            Time :  121.74121117591858\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  120.85\n",
      "Done for :1/135\n",
      "            Trial :  2\n",
      "            Time :  165.40946245193481\n",
      "            Vally :  [831, 861, 924, 957]\n",
      "            Avg_evaluation :  153.86\n",
      "Done for :2/135\n",
      "            Trial :  3\n",
      "            Time :  78.41466903686523\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  8.35\n",
      "Done for :3/135\n",
      "     Epsilon_Decay :  0.7\n",
      "            Trial :  1\n",
      "            Time :  167.08403635025024\n",
      "            Vally :  [885, 900, 981]\n",
      "            Avg_evaluation :  124.46\n",
      "Done for :4/135\n",
      "            Trial :  2\n",
      "            Time :  105.02711081504822\n",
      "            Vally :  [943, 967]\n",
      "            Avg_evaluation :  74.41\n",
      "Done for :5/135\n",
      "            Trial :  3\n",
      "            Time :  74.88621354103088\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  8.41\n",
      "Done for :6/135\n",
      "     Epsilon_Decay :  0.8\n",
      "            Trial :  1\n",
      "            Time :  123.08984041213989\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  57.49\n",
      "Done for :7/135\n",
      "            Trial :  2\n",
      "            Time :  74.95075511932373\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  7.98\n",
      "Done for :8/135\n",
      "            Trial :  3\n",
      "            Time :  123.95128774642944\n",
      "            Vally :  [920, 955]\n",
      "            Avg_evaluation :  97.88\n",
      "Done for :9/135\n",
      "     Epsilon_Decay :  0.9\n",
      "            Trial :  1\n",
      "            Time :  86.64674925804138\n",
      "            Vally :  [974, 978]\n",
      "            Avg_evaluation :  118.46\n",
      "Done for :10/135\n",
      "            Trial :  2\n",
      "            Time :  78.4317238330841\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  8.55\n",
      "Done for :11/135\n",
      "            Trial :  3\n",
      "            Time :  194.41829323768616\n",
      "            Vally :  [665, 863, 874, 875, 876, 877, 878, 879, 880, 910]\n",
      "            Avg_evaluation :  415.56\n",
      "Done for :12/135\n",
      "     Epsilon_Decay :  0.95\n",
      "            Trial :  1\n",
      "            Time :  209.36021423339844\n",
      "            Vally :  [850, 901, 974]\n",
      "            Avg_evaluation :  119.79\n",
      "Done for :13/135\n",
      "            Trial :  2\n",
      "            Time :  376.5655303001404\n",
      "            Vally :  [369, 563, 572, 596, 602, 643, 655, 657, 709, 732]\n",
      "            Avg_evaluation :  466.39\n",
      "Done for :14/135\n",
      "            Trial :  3\n",
      "            Time :  200.10557174682617\n",
      "            Vally :  [303, 337, 348, 351, 352, 353, 356, 446, 448, 455]\n",
      "            Avg_evaluation :  471.49\n",
      "Done for :15/135\n",
      "     Epsilon_Decay :  0.98\n",
      "            Trial :  1\n",
      "            Time :  321.02714920043945\n",
      "            Vally :  [282, 313, 455, 461, 462, 517, 518, 529, 542, 544]\n",
      "            Avg_evaluation :  368.24\n",
      "Done for :16/135\n",
      "            Trial :  2\n",
      "            Time :  227.20198130607605\n",
      "            Vally :  [142, 145, 197, 231, 235, 237, 238, 379, 405, 406]\n",
      "            Avg_evaluation :  485.35\n",
      "Done for :17/135\n",
      "            Trial :  3\n",
      "            Time :  750.5472812652588\n",
      "            Vally :  [391, 450, 626, 687, 736, 749, 877, 882, 922, 926]\n",
      "            Avg_evaluation :  236.53\n",
      "Done for :18/135\n",
      "     Epsilon_Decay :  0.985\n",
      "            Trial :  1\n",
      "            Time :  720.8133051395416\n",
      "            Vally :  [491, 498, 545, 587, 599, 610, 820, 872, 891, 913]\n",
      "            Avg_evaluation :  449.24\n",
      "Done for :19/135\n",
      "            Trial :  2\n",
      "            Time :  134.5279724597931\n",
      "            Vally :  [174, 196, 201, 205, 206, 208, 209, 211, 256, 260]\n",
      "            Avg_evaluation :  496.03\n",
      "Done for :20/135\n",
      "            Trial :  3\n",
      "            Time :  277.58879947662354\n",
      "            Vally :  [219, 295, 310, 312, 313, 389, 391, 400, 426, 438]\n",
      "            Avg_evaluation :  489.26\n",
      "Done for :21/135\n",
      "     Epsilon_Decay :  0.99\n",
      "            Trial :  1\n",
      "            Time :  388.9672272205353\n",
      "            Vally :  [308, 315, 328, 373, 447, 471, 477, 580, 588, 589]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :22/135\n",
      "            Trial :  2\n",
      "            Time :  189.82223081588745\n",
      "            Vally :  [297, 363, 370, 377, 385, 386, 388, 389, 391, 393]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :23/135\n",
      "            Trial :  3\n",
      "            Time :  152.7555296421051\n",
      "            Vally :  [242, 251, 252, 255, 256, 271, 273, 278, 293, 295]\n",
      "            Avg_evaluation :  481.75\n",
      "Done for :24/135\n",
      "     Epsilon_Decay :  0.995\n",
      "            Trial :  1\n",
      "            Time :  277.72813415527344\n",
      "            Vally :  [218, 336, 366, 368, 370, 371, 372, 376, 393, 429]\n",
      "            Avg_evaluation :  489.47\n",
      "Done for :25/135\n",
      "            Trial :  2\n",
      "            Time :  572.8928530216217\n",
      "            Vally :  [268, 483, 563, 569, 645, 655, 659, 684, 696, 718]\n",
      "            Avg_evaluation :  445.91\n",
      "Done for :26/135\n",
      "            Trial :  3\n",
      "            Time :  164.3379418849945\n",
      "            Vally :  [248, 254, 255, 281, 283, 288, 290, 300, 303, 305]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :27/135\n",
      "Done for gamma :  0.8\n",
      "[[[0.5, 0.8], [[121.74121117591858, [], 120.85, <__main__.DQNAgent object at 0x7f0312a69390>], [165.40946245193481, [831, 861, 924, 957], 153.86, <__main__.DQNAgent object at 0x7f02f43b52b0>], [78.41466903686523, [], 8.35, <__main__.DQNAgent object at 0x7f02ec6ff358>]]], [[0.7, 0.8], [[167.08403635025024, [885, 900, 981], 124.46, <__main__.DQNAgent object at 0x7f02ddab6b38>], [105.02711081504822, [943, 967], 74.41, <__main__.DQNAgent object at 0x7f02dd9c7e80>], [74.88621354103088, [], 8.41, <__main__.DQNAgent object at 0x7f02dd957128>]]], [[0.8, 0.8], [[123.08984041213989, [], 57.49, <__main__.DQNAgent object at 0x7f02de3647f0>], [74.95075511932373, [], 7.98, <__main__.DQNAgent object at 0x7f02ec72f3c8>], [123.95128774642944, [920, 955], 97.88, <__main__.DQNAgent object at 0x7f02dd7e4358>]]], [[0.9, 0.8], [[86.64674925804138, [974, 978], 118.46, <__main__.DQNAgent object at 0x7f02dd709828>], [78.4317238330841, [], 8.55, <__main__.DQNAgent object at 0x7f02dd698e10>], [194.41829323768616, [665, 863, 874, 875, 876, 877, 878, 879, 880, 910], 415.56, <__main__.DQNAgent object at 0x7f02dd62c550>]]], [[0.95, 0.8], [[209.36021423339844, [850, 901, 974], 119.79, <__main__.DQNAgent object at 0x7f02dd5509e8>], [376.5655303001404, [369, 563, 572, 596, 602, 643, 655, 657, 709, 732], 466.39, <__main__.DQNAgent object at 0x7f02dd4d6f98>], [200.10557174682617, [303, 337, 348, 351, 352, 353, 356, 446, 448, 455], 471.49, <__main__.DQNAgent object at 0x7f02dd472160>]]], [[0.98, 0.8], [[321.02714920043945, [282, 313, 455, 461, 462, 517, 518, 529, 542, 544], 368.24, <__main__.DQNAgent object at 0x7f02dd39dbe0>], [227.20198130607605, [142, 145, 197, 231, 235, 237, 238, 379, 405, 406], 485.35, <__main__.DQNAgent object at 0x7f02dd32c358>], [750.5472812652588, [391, 450, 626, 687, 736, 749, 877, 882, 922, 926], 236.53, <__main__.DQNAgent object at 0x7f02dd253828>]]], [[0.985, 0.8], [[720.8133051395416, [491, 498, 545, 587, 599, 610, 820, 872, 891, 913], 449.24, <__main__.DQNAgent object at 0x7f02dd1e5e10>], [134.5279724597931, [174, 196, 201, 205, 206, 208, 209, 211, 256, 260], 496.03, <__main__.DQNAgent object at 0x7f02dd179550>], [277.58879947662354, [219, 295, 310, 312, 313, 389, 391, 400, 426, 438], 489.26, <__main__.DQNAgent object at 0x7f02dd0a09e8>]]], [[0.99, 0.8], [[388.9672272205353, [308, 315, 328, 373, 447, 471, 477, 580, 588, 589], 499.0, <__main__.DQNAgent object at 0x7f02dd033f98>], [189.82223081588745, [297, 363, 370, 377, 385, 386, 388, 389, 391, 393], 499.0, <__main__.DQNAgent object at 0x7f02dcf44160>], [152.7555296421051, [242, 251, 252, 255, 256, 271, 273, 278, 293, 295], 481.75, <__main__.DQNAgent object at 0x7f02dceeabe0>]]], [[0.995, 0.8], [[277.72813415527344, [218, 336, 366, 368, 370, 371, 372, 376, 393, 429], 489.47, <__main__.DQNAgent object at 0x7f02dce7b358>], [572.8928530216217, [268, 483, 563, 569, 645, 655, 659, 684, 696, 718], 445.91, <__main__.DQNAgent object at 0x7f02dcda2828>], [164.3379418849945, [248, 254, 255, 281, 283, 288, 290, 300, 303, 305], 499.0, <__main__.DQNAgent object at 0x7f02dcd35e10>]]]]\n",
      "Gamma :  0.85\n",
      "     Epsilon_Decay :  0.5\n",
      "            Trial :  1\n",
      "            Time :  85.74650192260742\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  8.5\n",
      "Done for :28/135\n",
      "            Trial :  2\n",
      "            Time :  259.2677311897278\n",
      "            Vally :  [849, 850, 885, 906, 913, 925, 941]\n",
      "            Avg_evaluation :  203.88\n",
      "Done for :29/135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Trial :  3\n",
      "            Time :  110.5555329322815\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  25.25\n",
      "Done for :30/135\n",
      "     Epsilon_Decay :  0.7\n",
      "            Trial :  1\n",
      "            Time :  178.38320994377136\n",
      "            Vally :  [534, 535, 536, 537, 549, 550, 555, 556, 557, 559]\n",
      "            Avg_evaluation :  489.71\n",
      "Done for :31/135\n",
      "            Trial :  2\n",
      "            Time :  92.68818712234497\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  8.4\n",
      "Done for :32/135\n",
      "            Trial :  3\n",
      "            Time :  293.8559672832489\n",
      "            Vally :  [350, 353, 378, 386, 396, 398, 406, 409, 411, 438]\n",
      "            Avg_evaluation :  399.97\n",
      "Done for :33/135\n",
      "     Epsilon_Decay :  0.8\n",
      "            Trial :  1\n",
      "            Time :  562.1791880130768\n",
      "            Vally :  [725, 891, 968, 970, 979]\n",
      "            Avg_evaluation :  230.59\n",
      "Done for :34/135\n",
      "            Trial :  2\n",
      "            Time :  383.0894191265106\n",
      "            Vally :  [551, 591, 615, 625, 662, 681, 685, 699, 718, 723]\n",
      "            Avg_evaluation :  388.64\n",
      "Done for :35/135\n",
      "            Trial :  3\n",
      "            Time :  460.54130268096924\n",
      "            Vally :  [482, 598, 688, 733, 734, 735, 737, 741, 744, 751]\n",
      "            Avg_evaluation :  434.45\n",
      "Done for :36/135\n",
      "     Epsilon_Decay :  0.9\n",
      "            Trial :  1\n",
      "            Time :  261.47120428085327\n",
      "            Vally :  [586, 612, 659, 671, 684, 686, 687, 719, 724, 726]\n",
      "            Avg_evaluation :  327.3\n",
      "Done for :37/135\n",
      "            Trial :  2\n",
      "            Time :  523.0740597248077\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  169.84\n",
      "Done for :38/135\n",
      "            Trial :  3\n",
      "            Time :  364.81545186042786\n",
      "            Vally :  [692, 696, 736, 770, 786, 817, 893, 938, 940, 943]\n",
      "            Avg_evaluation :  230.6\n",
      "Done for :39/135\n",
      "     Epsilon_Decay :  0.95\n",
      "            Trial :  1\n",
      "            Time :  170.48540544509888\n",
      "            Vally :  [144, 253, 273, 274, 284, 285, 309, 311, 314, 316]\n",
      "            Avg_evaluation :  434.55\n",
      "Done for :40/135\n",
      "            Trial :  2\n",
      "            Time :  190.94813585281372\n",
      "            Vally :  [143, 330, 337, 339, 345, 347, 382, 384, 385, 387]\n",
      "            Avg_evaluation :  383.66\n",
      "Done for :41/135\n",
      "            Trial :  3\n",
      "            Time :  794.7931742668152\n",
      "            Vally :  [779]\n",
      "            Avg_evaluation :  152.91\n",
      "Done for :42/135\n",
      "     Epsilon_Decay :  0.98\n",
      "            Trial :  1\n",
      "            Time :  825.6403341293335\n",
      "            Vally :  [253, 301, 501, 503, 556]\n",
      "            Avg_evaluation :  131.22\n",
      "Done for :43/135\n",
      "            Trial :  2\n",
      "            Time :  196.71987223625183\n",
      "            Vally :  [281, 286, 293, 345, 353, 354, 359, 366, 369, 381]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :44/135\n",
      "            Trial :  3\n",
      "            Time :  341.07386660575867\n",
      "            Vally :  [335, 406, 407, 410, 455, 479, 481, 485, 511, 515]\n",
      "            Avg_evaluation :  446.79\n",
      "Done for :45/135\n",
      "     Epsilon_Decay :  0.985\n",
      "            Trial :  1\n",
      "            Time :  276.34618067741394\n",
      "            Vally :  [300, 338, 341, 342, 408, 428, 430, 442, 447, 452]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :46/135\n",
      "            Trial :  2\n",
      "            Time :  178.3313491344452\n",
      "            Vally :  [335, 383, 395, 398, 402, 421, 422, 424, 425, 426]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :47/135\n",
      "            Trial :  3\n",
      "            Time :  138.7398281097412\n",
      "            Vally :  [229, 278, 284, 285, 295, 297, 306, 307, 308, 309]\n",
      "            Avg_evaluation :  474.33\n",
      "Done for :48/135\n",
      "     Epsilon_Decay :  0.99\n",
      "            Trial :  1\n",
      "            Time :  153.36024689674377\n",
      "            Vally :  [228, 229, 231, 255, 257, 259, 262, 267, 268, 269]\n",
      "            Avg_evaluation :  485.32\n",
      "Done for :49/135\n",
      "            Trial :  2\n",
      "            Time :  220.49591827392578\n",
      "            Vally :  [251, 252, 253, 259, 286, 287, 302, 310, 311, 312]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :50/135\n",
      "            Trial :  3\n",
      "            Time :  208.40537571907043\n",
      "            Vally :  [298, 358, 361, 362, 365, 366, 367, 368, 369, 370]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :51/135\n",
      "     Epsilon_Decay :  0.995\n",
      "            Trial :  1\n",
      "            Time :  379.01351976394653\n",
      "            Vally :  [259, 260, 304, 347, 382, 406, 409, 410, 497, 505]\n",
      "            Avg_evaluation :  279.87\n",
      "Done for :52/135\n",
      "            Trial :  2\n",
      "            Time :  184.48010230064392\n",
      "            Vally :  [264, 293, 301, 303, 307, 308, 309, 313, 315, 316]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :53/135\n",
      "            Trial :  3\n",
      "            Time :  185.0844988822937\n",
      "            Vally :  [251, 283, 284, 297, 301, 304, 305, 308, 312, 315]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :54/135\n",
      "Done for gamma :  0.85\n",
      "[[[0.5, 0.85], [[85.74650192260742, [], 8.5, <__main__.DQNAgent object at 0x7f02dcc4c550>], [259.2677311897278, [849, 850, 885, 906, 913, 925, 941], 203.88, <__main__.DQNAgent object at 0x7f02dcbf39e8>], [110.5555329322815, [], 25.25, <__main__.DQNAgent object at 0x7f02dcb06f98>]]], [[0.7, 0.85], [[178.38320994377136, [534, 535, 536, 537, 549, 550, 555, 556, 557, 559], 489.71, <__main__.DQNAgent object at 0x7f02dca98160>], [92.68818712234497, [], 8.4, <__main__.DQNAgent object at 0x7f02dca3ebe0>], [293.8559672832489, [350, 353, 378, 386, 396, 398, 406, 409, 411, 438], 399.97, <__main__.DQNAgent object at 0x7f02dc94d358>]]], [[0.8, 0.85], [[562.1791880130768, [725, 891, 968, 970, 979], 230.59, <__main__.DQNAgent object at 0x7f02dc8f5828>], [383.0894191265106, [551, 591, 615, 625, 662, 681, 685, 699, 718, 723], 388.64, <__main__.DQNAgent object at 0x7f02dc809e10>], [460.54130268096924, [482, 598, 688, 733, 734, 735, 737, 741, 744, 751], 434.45, <__main__.DQNAgent object at 0x7f02dc79d550>]]], [[0.9, 0.85], [[261.47120428085327, [586, 612, 659, 671, 684, 686, 687, 719, 724, 726], 327.3, <__main__.DQNAgent object at 0x7f02dc6c49e8>], [523.0740597248077, [], 169.84, <__main__.DQNAgent object at 0x7f02dc657f98>], [364.81545186042786, [692, 696, 736, 770, 786, 817, 893, 938, 940, 943], 230.6, <__main__.DQNAgent object at 0x7f02dc5e7160>]]], [[0.95, 0.85], [[170.48540544509888, [144, 253, 273, 274, 284, 285, 309, 311, 314, 316], 434.55, <__main__.DQNAgent object at 0x7f02dc513be0>], [190.94813585281372, [143, 330, 337, 339, 345, 347, 382, 384, 385, 387], 383.66, <__main__.DQNAgent object at 0x7f02dc4a2358>], [794.7931742668152, [779], 152.91, <__main__.DQNAgent object at 0x7f02dc3c6828>]]], [[0.98, 0.85], [[825.6403341293335, [253, 301, 501, 503, 556], 131.22, <__main__.DQNAgent object at 0x7f02dc35ae10>], [196.71987223625183, [281, 286, 293, 345, 353, 354, 359, 366, 369, 381], 499.0, <__main__.DQNAgent object at 0x7f02dc2f0550>], [341.07386660575867, [335, 406, 407, 410, 455, 479, 481, 485, 511, 515], 446.79, <__main__.DQNAgent object at 0x7f02dc2169e8>]]], [[0.985, 0.85], [[276.34618067741394, [300, 338, 341, 342, 408, 428, 430, 442, 447, 452], 499.0, <__main__.DQNAgent object at 0x7f02dc1aaf98>], [178.3313491344452, [335, 383, 395, 398, 402, 421, 422, 424, 425, 426], 499.0, <__main__.DQNAgent object at 0x7f02dc13b160>], [138.7398281097412, [229, 278, 284, 285, 295, 297, 306, 307, 308, 309], 474.33, <__main__.DQNAgent object at 0x7f02dc063c18>]]], [[0.99, 0.85], [[153.36024689674377, [228, 229, 231, 255, 257, 259, 262, 267, 268, 269], 485.32, <__main__.DQNAgent object at 0x7f02bfff1358>], [220.49591827392578, [251, 252, 253, 259, 286, 287, 302, 310, 311, 312], 499.0, <__main__.DQNAgent object at 0x7f02bff187f0>], [208.40537571907043, [298, 358, 361, 362, 365, 366, 367, 368, 369, 370], 499.0, <__main__.DQNAgent object at 0x7f02bfeabe10>]]], [[0.995, 0.85], [[379.01351976394653, [259, 260, 304, 347, 382, 406, 409, 410, 497, 505], 279.87, <__main__.DQNAgent object at 0x7f02bfdc1518>], [184.48010230064392, [264, 293, 301, 303, 307, 308, 309, 313, 315, 316], 499.0, <__main__.DQNAgent object at 0x7f02bfd5d9e8>], [185.0844988822937, [251, 283, 284, 297, 301, 304, 305, 308, 312, 315], 499.0, <__main__.DQNAgent object at 0x7f02bfcfafd0>]]]]\n",
      "Gamma :  0.9\n",
      "     Epsilon_Decay :  0.5\n",
      "            Trial :  1\n",
      "            Time :  135.9234902858734\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  51.15\n",
      "Done for :55/135\n",
      "            Trial :  2\n",
      "            Time :  149.98998093605042\n",
      "            Vally :  [196, 213, 226, 227, 228, 230, 231, 232, 233, 234]\n",
      "            Avg_evaluation :  478.21\n",
      "Done for :56/135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Trial :  3\n",
      "            Time :  232.46793937683105\n",
      "            Vally :  [216, 270, 282, 286, 374, 379, 380, 381, 382, 383]\n",
      "            Avg_evaluation :  498.96\n",
      "Done for :57/135\n",
      "     Epsilon_Decay :  0.7\n",
      "            Trial :  1\n",
      "            Time :  241.41136622428894\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  98.16\n",
      "Done for :58/135\n",
      "            Trial :  2\n",
      "            Time :  395.5391414165497\n",
      "            Vally :  [338, 354, 368, 508, 530, 538, 544, 547, 554, 591]\n",
      "            Avg_evaluation :  408.29\n",
      "Done for :59/135\n",
      "            Trial :  3\n",
      "            Time :  119.20256638526917\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  13.14\n",
      "Done for :60/135\n",
      "     Epsilon_Decay :  0.8\n",
      "            Trial :  1\n",
      "            Time :  227.5701184272766\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  130.81\n",
      "Done for :61/135\n",
      "            Trial :  2\n",
      "            Time :  344.102525472641\n",
      "            Vally :  [300, 324, 326, 332, 333, 340, 342, 368, 370, 373]\n",
      "            Avg_evaluation :  423.14\n",
      "Done for :62/135\n",
      "            Trial :  3\n",
      "            Time :  714.8072266578674\n",
      "            Vally :  [895]\n",
      "            Avg_evaluation :  136.93\n",
      "Done for :63/135\n",
      "     Epsilon_Decay :  0.9\n",
      "            Trial :  1\n",
      "            Time :  344.41166281700134\n",
      "            Vally :  [625, 735, 765, 773, 782, 792, 818, 830, 832, 858]\n",
      "            Avg_evaluation :  475.2\n",
      "Done for :64/135\n",
      "            Trial :  2\n",
      "            Time :  310.46839213371277\n",
      "            Vally :  [141, 274, 275, 276, 344, 346, 350, 352, 355, 358]\n",
      "            Avg_evaluation :  436.52\n",
      "Done for :65/135\n",
      "            Trial :  3\n",
      "            Time :  129.01739954948425\n",
      "            Vally :  [141, 161, 162, 169, 185, 191, 215, 221, 235, 237]\n",
      "            Avg_evaluation :  154.6\n",
      "Done for :66/135\n",
      "     Epsilon_Decay :  0.95\n",
      "            Trial :  1\n",
      "            Time :  238.10275387763977\n",
      "            Vally :  [626, 633, 766, 768, 772, 778, 781, 783, 784, 785]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :67/135\n",
      "            Trial :  2\n",
      "            Time :  448.08448934555054\n",
      "            Vally :  [202, 466, 482, 488, 491, 493, 516, 558, 561, 579]\n",
      "            Avg_evaluation :  229.4\n",
      "Done for :68/135\n",
      "            Trial :  3\n",
      "            Time :  850.755654335022\n",
      "            Vally :  [394, 422, 651, 830, 902, 918, 921, 925, 942, 955]\n",
      "            Avg_evaluation :  402.48\n",
      "Done for :69/135\n",
      "     Epsilon_Decay :  0.98\n",
      "            Trial :  1\n",
      "            Time :  181.00702357292175\n",
      "            Vally :  [253, 282, 293, 294, 300, 301, 302, 303, 306, 308]\n",
      "            Avg_evaluation :  495.62\n",
      "Done for :70/135\n",
      "            Trial :  2\n",
      "            Time :  285.3468163013458\n",
      "            Vally :  [573, 631, 660, 674, 692, 693, 695, 696, 697, 698]\n",
      "            Avg_evaluation :  479.14\n",
      "Done for :71/135\n",
      "            Trial :  3\n",
      "            Time :  296.94163370132446\n",
      "            Vally :  [258, 266, 268, 269, 270, 271, 272, 273, 284, 326]\n",
      "            Avg_evaluation :  487.22\n",
      "Done for :72/135\n",
      "     Epsilon_Decay :  0.985\n",
      "            Trial :  1\n",
      "            Time :  530.0107574462891\n",
      "            Vally :  [323, 357, 440, 492, 509, 589, 602, 614, 619, 658]\n",
      "            Avg_evaluation :  365.12\n",
      "Done for :73/135\n",
      "            Trial :  2\n",
      "            Time :  164.81617760658264\n",
      "            Vally :  [156, 162, 164, 166, 177, 187, 196, 203, 204, 207]\n",
      "            Avg_evaluation :  492.77\n",
      "Done for :74/135\n",
      "            Trial :  3\n",
      "            Time :  532.0735745429993\n",
      "            Vally :  [305, 347, 357, 397, 433, 477, 482, 504, 509, 511]\n",
      "            Avg_evaluation :  415.9\n",
      "Done for :75/135\n",
      "     Epsilon_Decay :  0.99\n",
      "            Trial :  1\n",
      "            Time :  418.8403344154358\n",
      "            Vally :  [463, 478, 497, 504, 509, 514, 520, 523, 524, 533]\n",
      "            Avg_evaluation :  293.09\n",
      "Done for :76/135\n",
      "            Trial :  2\n",
      "            Time :  401.30847430229187\n",
      "            Vally :  [377, 381, 382, 388, 389, 391, 401, 420, 451, 458]\n",
      "            Avg_evaluation :  382.73\n",
      "Done for :77/135\n",
      "            Trial :  3\n",
      "            Time :  235.62946724891663\n",
      "            Vally :  [247, 254, 259, 264, 266, 270, 310, 315, 316, 317]\n",
      "            Avg_evaluation :  495.24\n",
      "Done for :78/135\n",
      "     Epsilon_Decay :  0.995\n",
      "            Trial :  1\n",
      "            Time :  290.0488455295563\n",
      "            Vally :  [329, 336, 340, 344, 345, 359, 360, 371, 372, 373]\n",
      "            Avg_evaluation :  494.77\n",
      "Done for :79/135\n",
      "            Trial :  2\n",
      "            Time :  427.5413420200348\n",
      "            Vally :  [280, 292, 331, 337, 339, 368, 369, 410, 429, 435]\n",
      "            Avg_evaluation :  486.79\n",
      "Done for :80/135\n",
      "            Trial :  3\n",
      "            Time :  226.59469294548035\n",
      "            Vally :  [290, 311, 330, 333, 334, 335, 336, 338, 339, 340]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :81/135\n",
      "Done for gamma :  0.9\n",
      "[[[0.5, 0.9], [[135.9234902858734, [], 51.15, <__main__.DQNAgent object at 0x7f02bfc0c198>], [149.98998093605042, [196, 213, 226, 227, 228, 230, 231, 232, 233, 234], 478.21, <__main__.DQNAgent object at 0x7f02bfbb6c18>], [232.46793937683105, [216, 270, 282, 286, 374, 379, 380, 381, 382, 383], 498.96, <__main__.DQNAgent object at 0x7f02bfac4358>]]], [[0.7, 0.9], [[241.41136622428894, [], 98.16, <__main__.DQNAgent object at 0x7f02bfa6b7f0>], [395.5391414165497, [338, 354, 368, 508, 530, 538, 544, 547, 554, 591], 408.29, <__main__.DQNAgent object at 0x7f02bf9fee10>], [119.20256638526917, [], 13.14, <__main__.DQNAgent object at 0x7f02bf913518>]]], [[0.8, 0.9], [[227.5701184272766, [], 130.81, <__main__.DQNAgent object at 0x7f02bf8bb9e8>], [344.102525472641, [300, 324, 326, 332, 333, 340, 342, 368, 370, 373], 423.14, <__main__.DQNAgent object at 0x7f02bf7cffd0>], [714.8072266578674, [895], 136.93, <__main__.DQNAgent object at 0x7f02bf75e198>]]], [[0.9, 0.9], [[344.41166281700134, [625, 735, 765, 773, 782, 792, 818, 830, 832, 858], 475.2, <__main__.DQNAgent object at 0x7f02bf688c18>], [310.46839213371277, [141, 274, 275, 276, 344, 346, 350, 352, 355, 358], 436.52, <__main__.DQNAgent object at 0x7f02bf616358>], [129.01739954948425, [141, 161, 162, 169, 185, 191, 215, 221, 235, 237], 154.6, <__main__.DQNAgent object at 0x7f02bf5bd7f0>]]], [[0.95, 0.9], [[238.10275387763977, [626, 633, 766, 768, 772, 778, 781, 783, 784, 785], 499.0, <__main__.DQNAgent object at 0x7f02bf4d1e10>], [448.08448934555054, [202, 466, 482, 488, 491, 493, 516, 558, 561, 579], 229.4, <__main__.DQNAgent object at 0x7f02bf463518>], [850.755654335022, [394, 422, 651, 830, 902, 918, 921, 925, 942, 955], 402.48, <__main__.DQNAgent object at 0x7f02bf38c9e8>]]], [[0.98, 0.9], [[181.00702357292175, [253, 282, 293, 294, 300, 301, 302, 303, 306, 308], 495.62, <__main__.DQNAgent object at 0x7f02bf320fd0>], [285.3468163013458, [573, 631, 660, 674, 692, 693, 695, 696, 697, 698], 479.14, <__main__.DQNAgent object at 0x7f02bf2b0198>], [296.94163370132446, [258, 266, 268, 269, 270, 271, 272, 273, 284, 326], 487.22, <__main__.DQNAgent object at 0x7f02bf1d8c18>]]], [[0.985, 0.9], [[530.0107574462891, [323, 357, 440, 492, 509, 589, 602, 614, 619, 658], 365.12, <__main__.DQNAgent object at 0x7f02bf16a320>], [164.81617760658264, [156, 162, 164, 166, 177, 187, 196, 203, 204, 207], 492.77, <__main__.DQNAgent object at 0x7f02bf08f7f0>], [532.0735745429993, [305, 347, 357, 397, 433, 477, 482, 504, 509, 511], 415.9, <__main__.DQNAgent object at 0x7f02bf021dd8>]]], [[0.99, 0.9], [[418.8403344154358, [463, 478, 497, 504, 509, 514, 520, 523, 524, 533], 293.09, <__main__.DQNAgent object at 0x7f02befb9550>], [401.30847430229187, [377, 381, 382, 388, 389, 391, 401, 420, 451, 458], 382.73, <__main__.DQNAgent object at 0x7f02beee0a20>], [235.62946724891663, [247, 254, 259, 264, 266, 270, 310, 315, 316, 317], 495.24, <__main__.DQNAgent object at 0x7f02bee6ffd0>]]], [[0.995, 0.9], [[290.0488455295563, [329, 336, 340, 344, 345, 359, 360, 371, 372, 373], 494.77, <__main__.DQNAgent object at 0x7f02bedfa198>], [427.5413420200348, [280, 292, 331, 337, 339, 368, 369, 410, 429, 435], 486.79, <__main__.DQNAgent object at 0x7f02bed2ac18>], [226.59469294548035, [290, 311, 330, 333, 334, 335, 336, 338, 339, 340], 499.0, <__main__.DQNAgent object at 0x7f02becbb320>]]]]\n",
      "Gamma :  0.95\n",
      "     Epsilon_Decay :  0.5\n",
      "            Trial :  1\n",
      "            Time :  657.491542339325\n",
      "            Vally :  [719, 740, 750, 801, 870, 968, 986, 989]\n",
      "            Avg_evaluation :  30.39\n",
      "Done for :82/135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Trial :  2\n",
      "            Time :  441.75407576560974\n",
      "            Vally :  [455, 474, 476, 481, 482, 491, 531, 533, 535, 536]\n",
      "            Avg_evaluation :  459.04\n",
      "Done for :83/135\n",
      "            Trial :  3\n",
      "            Time :  545.9164481163025\n",
      "            Vally :  [711, 757, 770, 777, 912, 915, 919, 928, 940, 944]\n",
      "            Avg_evaluation :  435.19\n",
      "Done for :84/135\n",
      "     Epsilon_Decay :  0.7\n",
      "            Trial :  1\n",
      "            Time :  473.8448796272278\n",
      "            Vally :  [768, 769, 773, 774]\n",
      "            Avg_evaluation :  148.73\n",
      "Done for :85/135\n",
      "            Trial :  2\n",
      "            Time :  796.1143538951874\n",
      "            Vally :  [625, 674, 683, 697, 755, 962, 963, 975, 992]\n",
      "            Avg_evaluation :  199.28\n",
      "Done for :86/135\n",
      "            Trial :  3\n",
      "            Time :  330.33265137672424\n",
      "            Vally :  [279, 293, 321, 371, 415, 439, 440, 441, 449, 451]\n",
      "            Avg_evaluation :  418.77\n",
      "Done for :87/135\n",
      "     Epsilon_Decay :  0.8\n",
      "            Trial :  1\n",
      "            Time :  645.9994218349457\n",
      "            Vally :  [908, 938, 949, 956, 975]\n",
      "            Avg_evaluation :  166.67\n",
      "Done for :88/135\n",
      "            Trial :  2\n",
      "            Time :  495.65130281448364\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  127.31\n",
      "Done for :89/135\n",
      "            Trial :  3\n",
      "            Time :  843.5580053329468\n",
      "            Vally :  [613, 657, 660, 664, 717, 803, 808, 819, 832, 864]\n",
      "            Avg_evaluation :  373.95\n",
      "Done for :90/135\n",
      "     Epsilon_Decay :  0.9\n",
      "            Trial :  1\n",
      "            Time :  515.0195717811584\n",
      "            Vally :  [847, 976]\n",
      "            Avg_evaluation :  158.34\n",
      "Done for :91/135\n",
      "            Trial :  2\n",
      "            Time :  525.8229250907898\n",
      "            Vally :  [752]\n",
      "            Avg_evaluation :  138.45\n",
      "Done for :92/135\n",
      "            Trial :  3\n",
      "            Time :  304.40091586112976\n",
      "            Vally :  [394, 425, 441, 443, 460, 461, 462, 477, 478, 480]\n",
      "            Avg_evaluation :  469.61\n",
      "Done for :93/135\n",
      "     Epsilon_Decay :  0.95\n",
      "            Trial :  1\n",
      "            Time :  509.55715131759644\n",
      "            Vally :  [454, 456, 471, 472, 488, 506, 524, 551, 601, 602]\n",
      "            Avg_evaluation :  463.86\n",
      "Done for :94/135\n",
      "            Trial :  2\n",
      "            Time :  246.5141544342041\n",
      "            Vally :  [241, 245, 255, 256, 284, 315, 325, 327, 328, 333]\n",
      "            Avg_evaluation :  276.26\n",
      "Done for :95/135\n",
      "            Trial :  3\n",
      "            Time :  180.53080558776855\n",
      "            Vally :  [267, 297, 308, 314, 319, 320, 322, 323, 324, 325]\n",
      "            Avg_evaluation :  450.85\n",
      "Done for :96/135\n",
      "     Epsilon_Decay :  0.98\n",
      "            Trial :  1\n",
      "            Time :  370.81509351730347\n",
      "            Vally :  [283, 298, 331, 356, 383, 402, 407, 444, 445, 477]\n",
      "            Avg_evaluation :  497.9\n",
      "Done for :97/135\n",
      "            Trial :  2\n",
      "            Time :  217.87477231025696\n",
      "            Vally :  [226, 240, 246, 252, 254, 256, 267, 280, 284, 287]\n",
      "            Avg_evaluation :  397.62\n",
      "Done for :98/135\n",
      "            Trial :  3\n",
      "            Time :  435.8446190357208\n",
      "            Vally :  [495, 497, 508, 515, 516, 519, 526, 530, 537, 547]\n",
      "            Avg_evaluation :  243.24\n",
      "Done for :99/135\n",
      "     Epsilon_Decay :  0.985\n",
      "            Trial :  1\n",
      "            Time :  200.40296411514282\n",
      "            Vally :  [267, 282, 283, 284, 285, 290, 297, 303, 304, 305]\n",
      "            Avg_evaluation :  495.08\n",
      "Done for :100/135\n",
      "            Trial :  2\n",
      "            Time :  948.2428302764893\n",
      "            Vally :  [595, 619, 792, 815, 935]\n",
      "            Avg_evaluation :  148.93\n",
      "Done for :101/135\n",
      "            Trial :  3\n",
      "            Time :  241.36285758018494\n",
      "            Vally :  [221, 239, 241, 244, 245, 247, 249, 253, 255, 256]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :102/135\n",
      "     Epsilon_Decay :  0.99\n",
      "            Trial :  1\n",
      "            Time :  327.0197329521179\n",
      "            Vally :  [257, 262, 299, 302, 310, 313, 314, 323, 325, 347]\n",
      "            Avg_evaluation :  463.84\n",
      "Done for :103/135\n",
      "            Trial :  2\n",
      "            Time :  299.37802481651306\n",
      "            Vally :  [213, 245, 246, 279, 280, 284, 285, 289, 291, 294]\n",
      "            Avg_evaluation :  366.99\n",
      "Done for :104/135\n",
      "            Trial :  3\n",
      "            Time :  2507.8673915863037\n",
      "            Vally :  [290, 412, 429, 430, 482, 534, 556, 557, 559, 560]\n",
      "            Avg_evaluation :  476.48\n",
      "Done for :105/135\n",
      "     Epsilon_Decay :  0.995\n",
      "            Trial :  1\n",
      "            Time :  341.2370753288269\n",
      "            Vally :  [236, 254, 257, 260, 268, 271, 273, 276, 278, 279]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :106/135\n",
      "            Trial :  2\n",
      "            Time :  594.6714022159576\n",
      "            Vally :  [332, 417, 423, 431, 432, 433, 434, 435, 436, 437]\n",
      "            Avg_evaluation :  499.0\n",
      "Done for :107/135\n",
      "            Trial :  3\n",
      "            Time :  462.9580361843109\n",
      "            Vally :  [246, 273, 278, 281, 294, 307, 314, 317, 318, 332]\n",
      "            Avg_evaluation :  485.23\n",
      "Done for :108/135\n",
      "Done for gamma :  0.95\n",
      "[[[0.5, 0.95], [[657.491542339325, [719, 740, 750, 801, 870, 968, 986, 989], 30.39, <__main__.DQNAgent object at 0x7f02bebe37f0>], [441.75407576560974, [455, 474, 476, 481, 482, 491, 531, 533, 535, 536], 459.04, <__main__.DQNAgent object at 0x7f02beb75dd8>], [545.9164481163025, [711, 757, 770, 777, 912, 915, 919, 928, 940, 944], 435.19, <__main__.DQNAgent object at 0x7f02bea89550>]]], [[0.7, 0.95], [[473.8448796272278, [768, 769, 773, 774], 148.73, <__main__.DQNAgent object at 0x7f02bea31a20>], [796.1143538951874, [625, 674, 683, 697, 755, 962, 963, 975, 992], 199.28, <__main__.DQNAgent object at 0x7f02be943fd0>], [330.33265137672424, [279, 293, 321, 371, 415, 439, 440, 441, 449, 451], 418.77, <__main__.DQNAgent object at 0x7f02be8d6198>]]], [[0.8, 0.95], [[645.9994218349457, [908, 938, 949, 956, 975], 166.67, <__main__.DQNAgent object at 0x7f02be87ec18>], [495.65130281448364, [], 127.31, <__main__.DQNAgent object at 0x7f02be78f320>], [843.5580053329468, [613, 657, 660, 664, 717, 803, 808, 819, 832, 864], 373.95, <__main__.DQNAgent object at 0x7f02be7357f0>]]], [[0.9, 0.95], [[515.0195717811584, [847, 976], 158.34, <__main__.DQNAgent object at 0x7f02be647dd8>], [525.8229250907898, [752], 138.45, <__main__.DQNAgent object at 0x7f02be5d4550>], [304.40091586112976, [394, 425, 441, 443, 460, 461, 462, 477, 478, 480], 469.61, <__main__.DQNAgent object at 0x7f02be502a20>]]], [[0.95, 0.95], [[509.55715131759644, [454, 456, 471, 472, 488, 506, 524, 551, 601, 602], 463.86, <__main__.DQNAgent object at 0x7f02be494fd0>], [246.5141544342041, [241, 245, 255, 256, 284, 315, 325, 327, 328, 333], 276.26, <__main__.DQNAgent object at 0x7f02be426198>], [180.53080558776855, [267, 297, 308, 314, 319, 320, 322, 323, 324, 325], 450.85, <__main__.DQNAgent object at 0x7f02be34ec18>]]], [[0.98, 0.95], [[370.81509351730347, [283, 298, 331, 356, 383, 402, 407, 444, 445, 477], 497.9, <__main__.DQNAgent object at 0x7f02be2e0320>], [217.87477231025696, [226, 240, 246, 252, 254, 256, 267, 280, 284, 287], 397.62, <__main__.DQNAgent object at 0x7f02be2067f0>], [435.8446190357208, [495, 497, 508, 515, 516, 519, 526, 530, 537, 547], 243.24, <__main__.DQNAgent object at 0x7f02be197dd8>]]], [[0.985, 0.95], [[200.40296411514282, [267, 282, 283, 284, 285, 290, 297, 303, 304, 305], 495.08, <__main__.DQNAgent object at 0x7f02be12f550>], [948.2428302764893, [595, 619, 792, 815, 935], 148.93, <__main__.DQNAgent object at 0x7f02be055a20>], [241.36285758018494, [221, 239, 241, 244, 245, 247, 249, 253, 255, 256], 499.0, <__main__.DQNAgent object at 0x7f02bdfe6f60>]]], [[0.99, 0.95], [[327.0197329521179, [257, 262, 299, 302, 310, 313, 314, 323, 325, 347], 463.84, <__main__.DQNAgent object at 0x7f02bdf7c0b8>], [299.37802481651306, [213, 245, 246, 279, 280, 284, 285, 289, 291, 294], 366.99, <__main__.DQNAgent object at 0x7f02bdea4ac8>], [2507.8673915863037, [290, 412, 429, 430, 482, 534, 556, 557, 559, 560], 476.48, <__main__.DQNAgent object at 0x7f02bde2de48>]]], [[0.995, 0.95], [[341.2370753288269, [236, 254, 257, 260, 268, 271, 273, 276, 278, 279], 499.0, <__main__.DQNAgent object at 0x7f02bdd47160>], [594.6714022159576, [332, 417, 423, 431, 432, 433, 434, 435, 436, 437], 499.0, <__main__.DQNAgent object at 0x7f02bdcf2b38>], [462.9580361843109, [246, 273, 278, 281, 294, 307, 314, 317, 318, 332], 485.23, <__main__.DQNAgent object at 0x7f02bdc01ef0>]]]]\n",
      "Gamma :  1\n",
      "     Epsilon_Decay :  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Trial :  1\n",
      "            Time :  901.4009943008423\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  26.6\n",
      "Done for :109/135\n",
      "            Trial :  2\n",
      "            Time :  269.86127400398254\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  71.12\n",
      "Done for :110/135\n",
      "            Trial :  3\n",
      "            Time :  323.1885316371918\n",
      "            Vally :  [448, 522, 543, 549, 552, 584, 589, 590, 591, 592]\n",
      "            Avg_evaluation :  329.38\n",
      "Done for :111/135\n",
      "     Epsilon_Decay :  0.7\n",
      "            Trial :  1\n",
      "            Time :  297.24556517601013\n",
      "            Vally :  [312, 342, 414, 422, 424, 451, 452, 454, 455, 457]\n",
      "            Avg_evaluation :  490.19\n",
      "Done for :112/135\n",
      "            Trial :  2\n",
      "            Time :  717.5834314823151\n",
      "            Vally :  [956, 998]\n",
      "            Avg_evaluation :  335.94\n",
      "Done for :113/135\n",
      "            Trial :  3\n",
      "            Time :  214.11640644073486\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  12.16\n",
      "Done for :114/135\n",
      "     Epsilon_Decay :  0.8\n",
      "            Trial :  1\n",
      "            Time :  241.08404231071472\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  19.52\n",
      "Done for :115/135\n",
      "            Trial :  2\n",
      "            Time :  172.73356246948242\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  8.83\n",
      "Done for :116/135\n",
      "            Trial :  3\n",
      "            Time :  210.12489533424377\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  8.3\n",
      "Done for :117/135\n",
      "     Epsilon_Decay :  0.9\n",
      "            Trial :  1\n",
      "            Time :  1100.5431427955627\n",
      "            Vally :  [627, 630, 688, 745, 827, 919, 939]\n",
      "            Avg_evaluation :  173.74\n",
      "Done for :118/135\n",
      "            Trial :  2\n",
      "            Time :  261.92259407043457\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  10.93\n",
      "Done for :119/135\n",
      "            Trial :  3\n",
      "            Time :  235.65430688858032\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  12.46\n",
      "Done for :120/135\n",
      "     Epsilon_Decay :  0.95\n",
      "            Trial :  1\n",
      "            Time :  675.2478775978088\n",
      "            Vally :  [852, 856, 861, 862, 863, 872, 875, 890, 895, 896]\n",
      "            Avg_evaluation :  372.46\n",
      "Done for :121/135\n",
      "            Trial :  2\n",
      "            Time :  1009.1629030704498\n",
      "            Vally :  [510, 521, 522, 707, 786, 936, 937, 949]\n",
      "            Avg_evaluation :  121.28\n",
      "Done for :122/135\n",
      "            Trial :  3\n",
      "            Time :  285.9170837402344\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  8.3\n",
      "Done for :123/135\n",
      "     Epsilon_Decay :  0.98\n",
      "            Trial :  1\n",
      "            Time :  800.5454738140106\n",
      "            Vally :  [836, 837, 838, 839, 843, 844, 845, 852, 877, 880]\n",
      "            Avg_evaluation :  430.34\n",
      "Done for :124/135\n",
      "            Trial :  2\n",
      "            Time :  452.5989832878113\n",
      "            Vally :  [884, 927, 930, 935, 939, 942, 944, 950, 951, 953]\n",
      "            Avg_evaluation :  440.11\n",
      "Done for :125/135\n",
      "            Trial :  3\n",
      "            Time :  1223.63401055336\n",
      "            Vally :  [735, 808, 971]\n",
      "            Avg_evaluation :  254.8\n",
      "Done for :126/135\n",
      "     Epsilon_Decay :  0.985\n",
      "            Trial :  1\n",
      "            Time :  266.01434779167175\n",
      "            Vally :  [401, 403, 404, 406, 407, 533, 567, 573, 576, 582]\n",
      "            Avg_evaluation :  259.1\n",
      "Done for :127/135\n",
      "            Trial :  2\n",
      "            Time :  1016.8221802711487\n",
      "            Vally :  [477]\n",
      "            Avg_evaluation :  124.14\n",
      "Done for :128/135\n",
      "            Trial :  3\n",
      "            Time :  2415.443090438843\n",
      "            Vally :  [722, 762]\n",
      "            Avg_evaluation :  179.55\n",
      "Done for :129/135\n",
      "     Epsilon_Decay :  0.99\n",
      "            Trial :  1\n",
      "            Time :  456.72077655792236\n",
      "            Vally :  [744]\n",
      "            Avg_evaluation :  149.32\n",
      "Done for :130/135\n",
      "            Trial :  2\n",
      "            Time :  657.9459209442139\n",
      "            Vally :  [888, 947, 957, 974, 982]\n",
      "            Avg_evaluation :  138.37\n",
      "Done for :131/135\n",
      "            Trial :  3\n",
      "            Time :  435.9719486236572\n",
      "            Vally :  [675, 676, 692, 693, 699, 701, 706, 716, 721, 724]\n",
      "            Avg_evaluation :  475.62\n",
      "Done for :132/135\n",
      "     Epsilon_Decay :  0.995\n",
      "            Trial :  1\n",
      "            Time :  206.82336926460266\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  8.42\n",
      "Done for :133/135\n",
      "            Trial :  2\n",
      "            Time :  1351.620236158371\n",
      "            Vally :  [502, 912, 961, 965, 968, 974, 975, 980, 981, 993]\n",
      "            Avg_evaluation :  366.08\n",
      "Done for :134/135\n",
      "            Trial :  3\n",
      "            Time :  245.02494359016418\n",
      "            Vally :  []\n",
      "            Avg_evaluation :  9.78\n",
      "Done for :135/135\n",
      "Done for gamma :  1\n",
      "[[[0.5, 1], [[901.4009943008423, [], 26.6, <__main__.DQNAgent object at 0x7f02bd9a66a0>], [269.86127400398254, [], 71.12, <__main__.DQNAgent object at 0x7f02bd93dbe0>], [323.1885316371918, [448, 522, 543, 549, 552, 584, 589, 590, 591, 592], 329.38, <__main__.DQNAgent object at 0x7f02bd850f98>]]], [[0.7, 1], [[297.24556517601013, [312, 342, 414, 422, 424, 451, 452, 454, 455, 457], 490.19, <__main__.DQNAgent object at 0x7f02bd7f4748>], [717.5834314823151, [956, 998], 335.94, <__main__.DQNAgent object at 0x7f02bd70ccc0>], [214.11640644073486, [], 12.16, <__main__.DQNAgent object at 0x7f02bd69d390>]]], [[0.8, 1], [[241.08404231071472, [], 19.52, <__main__.DQNAgent object at 0x7f02bd5c37b8>], [172.73356246948242, [], 8.83, <__main__.DQNAgent object at 0x7f02bd558d68>], [210.12489533424377, [], 8.3, <__main__.DQNAgent object at 0x7f02bd4eb400>]]], [[0.9, 1], [[1100.5431427955627, [627, 630, 688, 745, 827, 919, 939], 173.74, <__main__.DQNAgent object at 0x7f02bd40c860>], [261.92259407043457, [], 10.93, <__main__.DQNAgent object at 0x7f02bd3a7dd8>], [235.65430688858032, [], 12.46, <__main__.DQNAgent object at 0x7f02bd3384e0>]]], [[0.95, 1], [[675.2478775978088, [852, 856, 861, 862, 863, 872, 875, 890, 895, 896], 372.46, <__main__.DQNAgent object at 0x7f02bd263940>], [1009.1629030704498, [510, 521, 522, 707, 786, 936, 937, 949], 121.28, <__main__.DQNAgent object at 0x7f02bd1f4e80>], [285.9170837402344, [], 8.3, <__main__.DQNAgent object at 0x7f02bd107588>]]], [[0.98, 1], [[800.5454738140106, [836, 837, 838, 839, 843, 844, 845, 852, 877, 880], 430.34, <__main__.DQNAgent object at 0x7f02bd0b09e8>], [452.5989832878113, [884, 927, 930, 935, 939, 942, 944, 950, 951, 953], 440.11, <__main__.DQNAgent object at 0x7f02bcfc2f60>], [1223.63401055336, [735, 808, 971], 254.8, <__main__.DQNAgent object at 0x7f02bcf57208>]]], [[0.985, 1], [[266.01434779167175, [401, 403, 404, 406, 407, 533, 567, 573, 576, 582], 259.1, <__main__.DQNAgent object at 0x7f02bce80a58>], [1016.8221802711487, [477], 124.14, <__main__.DQNAgent object at 0x7f02bce12f98>], [2415.443090438843, [722, 762], 179.55, <__main__.DQNAgent object at 0x7f02bcda30f0>]]], [[0.99, 1], [[456.72077655792236, [744], 149.32, <__main__.DQNAgent object at 0x7f02bccccb00>], [657.9459209442139, [888, 947, 957, 974, 982], 138.37, <__main__.DQNAgent object at 0x7f02bcc60eb8>], [435.9719486236572, [675, 676, 692, 693, 699, 701, 706, 716, 721, 724], 475.62, <__main__.DQNAgent object at 0x7f02bcb81668>]]], [[0.995, 1], [[206.82336926460266, [], 8.42, <__main__.DQNAgent object at 0x7f02bcb1abe0>], [1351.620236158371, [502, 912, 961, 965, 968, 974, 975, 980, 981, 993], 366.08, <__main__.DQNAgent object at 0x7f02bcaabf60>], [245.02494359016418, [], 9.78, <__main__.DQNAgent object at 0x7f02bc9d16d8>]]]]\n"
     ]
    }
   ],
   "source": [
    "agent_hyp_perf=[]\n",
    "crt=0\n",
    "for i in range(len(gammaLst)):\n",
    "    print('Gamma : ',gammaLst[i] )\n",
    "    agent_gamma_perf=[]\n",
    "    for j in range(len(epsilon_decayLst)):\n",
    "        print('     Epsilon_Decay : ',epsilon_decayLst[j] )\n",
    "        finl_perf=[]\n",
    "        for po in range(3):\n",
    "            finl_perf.append(do_it(agent_hyp[i][j][po]))\n",
    "            print('            Trial : ',po+1 )\n",
    "            print('            Time : ',finl_perf[po][0] )\n",
    "            print('            Vally : ',finl_perf[po][1] )\n",
    "            print('            Avg_evaluation : ',finl_perf[po][2] )\n",
    "            crt+=1\n",
    "            print('Done for :'+str(crt)+'/135')\n",
    "        agent_gamma_perf.append([[epsilon_decayLst[j],gammaLst[i]],finl_perf])\n",
    "    agent_hyp_perf.append(agent_gamma_perf)\n",
    "    print(\"Done for gamma : \",gammaLst[i])\n",
    "    print(agent_gamma_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_root='CartPole/Phase-3/Models'\n",
    "for i in range(len(gammaLst)):\n",
    "    for j in range(len(epsilon_decayLst)):\n",
    "        for po in range(3):\n",
    "            agent_hyp[i][j][po].save(os.path.join(data_root,'G'+str(gammaLst[i])+'ED'+str(epsilon_decayLst[j])+'trail'+str(po)+'.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
